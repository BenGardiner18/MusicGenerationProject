{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DS340 Final Project - Using ML to generate music\n",
        "Ben Gardiner"
      ],
      "metadata": {
        "id": "uLRTsnVfatCc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJhZN_lrAssf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "# import sklearn\n",
        "!pip install pretty_midi\n",
        "import pretty_midi\n",
        "from tqdm import tqdm\n",
        "import random \n",
        "import re\n",
        "import collections \n",
        "import pathlib \n",
        "import glob \n",
        "warnings.simplefilter(\"ignore\", UserWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "For this assignment I wanted to generate unique artifical music using machine learning. Music can be represented a sequence of sounds following each other, so I collected data in time series format and used a LSTM model to predict the sound that would come next."
      ],
      "metadata": {
        "id": "f-GBT3j5kis-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "1. get data \n",
        "2. preprocess data \n",
        "3. build modeel\n",
        "4. train it \n",
        "5. generate new music"
      ],
      "metadata": {
        "id": "0a6NZ2wplHte"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6TEAPSxfG2B"
      },
      "source": [
        "# Getting data \n",
        "\n",
        "The Maestro dataset is a collection of over 200 hours of classical piano performances in MIDI and audio formats, spanning a wide range of musical styles and composers. It includes annotations such as tempo, dynamics, and pedal usage, and is commonly used for machine learning, music information retrieval, and computer music research. It can be downloaded for non-commercial use from the Magenta project website.\n",
        "\n",
        "https://magenta.tensorflow.org/datasets/maestro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SsxyF9pfG2C"
      },
      "outputs": [],
      "source": [
        "data_dir = pathlib.Path('data/maestro-v2.0.0')\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'maestro-v2.0.0-midi.zip',\n",
        "      origin='https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip',\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data',\n",
        "  )\n",
        "\n",
        "midi_paths = glob.glob(str(data_dir/'**/*.mid*'))\n",
        "print('Number of files:', len(midi_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4fHa7d3Br45"
      },
      "outputs": [],
      "source": [
        "sample_file = midi_paths[0]\n",
        "sample_object = pretty_midi.PrettyMIDI(sample_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27FaWoX9fG2D"
      },
      "source": [
        "# Preprocessing our data\n",
        "\n",
        "Right now our data is a bunch of MIDI files stored in our directory, but we need to transform these files into something the neural network can consume. We will preprocess the data with the following steps...\n",
        "\n",
        "1. take a MIDI object and turn it into a sequence of notes \n",
        "2. visualize the song by plotting a sequence of notes \n",
        "3. iterate through all the songs in the dataset and collect song data (sequence of notes, name of song, length of second (seconds), etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a-xhn1o74mR"
      },
      "outputs": [],
      "source": [
        "\n",
        "def midi_to_notes(midi_object):\n",
        "  # source - https://www.tensorflow.org/tutorials/audio/music_generation\n",
        "  instrument = midi_object.instruments[0]\n",
        "  notes = collections.defaultdict(list)\n",
        "\n",
        "  # Sort the notes by start time\n",
        "  sorted_notes = sorted(instrument.notes, key=lambda note: note.start)\n",
        "  prev_start = sorted_notes[0].start\n",
        "  \n",
        "  for note in sorted_notes:\n",
        "    start = note.start\n",
        "    end = note.end\n",
        "    pitch = note.pitch\n",
        "    notes['pitch'].append(pitch)\n",
        "    notes['note_name'].append(pretty_midi.note_number_to_name(pitch)) # e.g 50 -> D3\n",
        "    notes['start'].append(start)\n",
        "    notes['end'].append(end)\n",
        "    notes['step'].append(start - prev_start)\n",
        "    notes['duration'].append(end - start)\n",
        "    prev_start = start\n",
        "\n",
        "  return pd.DataFrame({name: np.array(value) for name, value in notes.items()})\n",
        "\n",
        "midi_to_notes(sample_object)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeXnwLYl74mR"
      },
      "outputs": [],
      "source": [
        "def plot_piano_roll(notes: pd.DataFrame, count: int = None):\n",
        "  \"\"\"Used to visualize the notes in a track.\"\"\"\n",
        "  # source - https://www.tensorflow.org/tutorials/audio/music_generation\n",
        "\n",
        "  if count:\n",
        "    title = f'First {count} notes'\n",
        "  else:\n",
        "    title = f'Whole track'\n",
        "    count = len(notes['pitch'])\n",
        "  plt.figure(figsize=(20, 4))\n",
        "  plot_pitch = np.stack([notes['pitch'], notes['pitch']], axis=0)\n",
        "  plot_start_stop = np.stack([notes['start'], notes['end']], axis=0)\n",
        "  plt.plot(\n",
        "      plot_start_stop[:, :count], plot_pitch[:, :count], color=\"b\", marker=\".\")\n",
        "  plt.xlabel('Time [s]')\n",
        "  plt.ylabel('Pitch')\n",
        "  _ = plt.title(title)\n",
        "\n",
        "plot_piano_roll(midi_to_notes(sample_object),count=10)\n",
        "plot_piano_roll(midi_to_notes(sample_object))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi3Wc3DH74mS"
      },
      "outputs": [],
      "source": [
        "def get_song_data(file_path):\n",
        "    \"\"\"Returns song name, length, number of notes, and vectorized notes for an INDIVIDUAL song\"\"\"\n",
        "    midi_object = pretty_midi.PrettyMIDI(file_path)\n",
        "\n",
        "    # length of song in seconds\n",
        "    song_length = midi_object.get_end_time()\n",
        "\n",
        "    # name of song\n",
        "    song_name = os.path.basename(file_path)\n",
        "\n",
        "    # notes df \n",
        "    notes_df = midi_to_notes(midi_object)\n",
        "    num_notes = len(notes_df)\n",
        "\n",
        "    # sequences of notes in array form e.g [50, 51, 52, 53, 54]\n",
        "    vectorized_notes = notes_df[['pitch','step','duration']].values\n",
        "\n",
        "    return song_name, song_length, num_notes, vectorized_notes\n",
        "    \n",
        "get_song_data(sample_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "epZ7LjRn74mS",
        "outputId": "6cb3711a-1c03-4781-c894-042e23def97e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 7/1282 [00:05<13:10,  1.61it/s]Exception ignored in: <function _xla_gc_callback at 0x7f97d5ad7820>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/jax/_src/lib/__init__.py\", line 97, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n",
            "  1%|          | 14/1282 [00:06<07:42,  2.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error with data/maestro-v2.0.0/2009/MIDI-Unprocessed_13_R1_2009_04_ORIG_MID--AUDIO_13_R1_2009_13_R1_2009_04_WAV.midi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1282/1282 [07:40<00:00,  2.78it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           song name  song length (seconds)  \\\n",
              "0  MIDI-Unprocessed_Recital20_MID--AUDIO_20_R1_20...            2098.052083   \n",
              "1  MIDI-Unprocessed_13_R1_2008_01-04_ORIG_MID--AU...             667.652344   \n",
              "2  MIDI-Unprocessed_R2_D2-12-13-15_mid--AUDIO-fro...             647.133333   \n",
              "3  MIDI-Unprocessed_Chamber4_MID--AUDIO_11_R3_201...            1091.661458   \n",
              "4  MIDI-Unprocessed_XP_09_R1_2004_05_ORIG_MID--AU...             768.143750   \n",
              "\n",
              "   number of notes                                   vectorized_notes  \n",
              "0            23835  [[73.0, 0.0, 0.38020833333333326], [64.0, 0.05...  \n",
              "1             7886  [[81.0, 0.0, 0.08333333333333326], [76.0, 0.04...  \n",
              "2             8775  [[46.0, 0.0, 0.10208333333333353], [34.0, 0.01...  \n",
              "3             8038  [[50.0, 0.0, 1.0625], [74.0, 0.006510416666666...  \n",
              "4             6699  [[39.0, 0.0, 0.10312499999999991], [51.0, 0.00...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3e22ebe4-5a5d-4592-8e78-8d74dbea3cd6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>song name</th>\n",
              "      <th>song length (seconds)</th>\n",
              "      <th>number of notes</th>\n",
              "      <th>vectorized_notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MIDI-Unprocessed_Recital20_MID--AUDIO_20_R1_20...</td>\n",
              "      <td>2098.052083</td>\n",
              "      <td>23835</td>\n",
              "      <td>[[73.0, 0.0, 0.38020833333333326], [64.0, 0.05...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MIDI-Unprocessed_13_R1_2008_01-04_ORIG_MID--AU...</td>\n",
              "      <td>667.652344</td>\n",
              "      <td>7886</td>\n",
              "      <td>[[81.0, 0.0, 0.08333333333333326], [76.0, 0.04...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>MIDI-Unprocessed_R2_D2-12-13-15_mid--AUDIO-fro...</td>\n",
              "      <td>647.133333</td>\n",
              "      <td>8775</td>\n",
              "      <td>[[46.0, 0.0, 0.10208333333333353], [34.0, 0.01...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MIDI-Unprocessed_Chamber4_MID--AUDIO_11_R3_201...</td>\n",
              "      <td>1091.661458</td>\n",
              "      <td>8038</td>\n",
              "      <td>[[50.0, 0.0, 1.0625], [74.0, 0.006510416666666...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MIDI-Unprocessed_XP_09_R1_2004_05_ORIG_MID--AU...</td>\n",
              "      <td>768.143750</td>\n",
              "      <td>6699</td>\n",
              "      <td>[[39.0, 0.0, 0.10312499999999991], [51.0, 0.00...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e22ebe4-5a5d-4592-8e78-8d74dbea3cd6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3e22ebe4-5a5d-4592-8e78-8d74dbea3cd6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3e22ebe4-5a5d-4592-8e78-8d74dbea3cd6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "def get_data(midi_paths):\n",
        "    data = []\n",
        "    for path in tqdm(midi_paths):\n",
        "        try:\n",
        "            song_name, song_length, num_notes, vectorized_notes = get_song_data(path)\n",
        "            song_data = [song_name, song_length, num_notes, vectorized_notes]\n",
        "            \n",
        "            # # we only want songs that are less than 5 minutes\n",
        "            # if song_length < 300: \n",
        "            #     data.append(song_data)\n",
        "            data.append(song_data)\n",
        "\n",
        "            # else:\n",
        "            #     continue\n",
        "        except:\n",
        "            print(f\"Error with {path}\")\n",
        "\n",
        "    df = pd.DataFrame(data, columns=[\"song name\", \"song length (seconds)\", \"number of notes\", \"vectorized_notes\"])\n",
        "    # print(df.head())\n",
        "\n",
        "    all_notes = df['vectorized_notes']\n",
        "\n",
        "    return all_notes,df\n",
        "\n",
        "percent = 1\n",
        "num_files = int(len(midi_paths)*percent)\n",
        "random_paths = random.sample(midi_paths,num_files)\n",
        "all_notes, df = get_data(random_paths)\n",
        "# all_notes, df = get_data(midi_paths[:5])\n",
        "\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a33OsIRMD4AP"
      },
      "source": [
        "### Creating input and output sequences\n",
        "\n",
        "Right now our data is a list of songs where each song is a sequence of notes. However, we need to reshape our data into something the model can understand, so we need to create input and output sequences.\n",
        "\n",
        "We iterate through all the data and collect 64 note sequences and the note that follows. We'll train the model on the 64 note sequences and hope it learns what note should follow. Just like you could use a time series model to predict where a stock might be based on where it was in the past."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Hqj9ICg3fG2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "904674b5-7102-4103-dfee-492e5f568043"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequences\n",
            "<class 'numpy.ndarray'>\n",
            "(7075850, 25, 3)\n",
            "Output Sequences\n",
            "<class 'numpy.ndarray'>\n",
            "(7075850, 3)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def create_input_output(data, sequence_length=25):\n",
        "    input_seqs = []\n",
        "    output_notes = []\n",
        "\n",
        "    for song in data:\n",
        "        for i in range(0, len(song) - sequence_length):\n",
        "            input_seq = song[i:i+sequence_length]\n",
        "            output_note = song[i+sequence_length]\n",
        "\n",
        "            input_seqs.append(input_seq)\n",
        "            output_notes.append(output_note)\n",
        "\n",
        "    input_seqs = np.array(input_seqs)\n",
        "    output_seqs = np.array([\n",
        "        # to_categorical(np.array([note[0] for note in output_notes])), # pitch\n",
        "        np.array([note[0] for note in output_notes]), # pitch\n",
        "        np.array([note[1] for note in output_notes]), # step\n",
        "        np.array([note[2] for note in output_notes]) # duration\n",
        "    ]).transpose()\n",
        "\n",
        "    return input_seqs, output_seqs\n",
        "\n",
        "input_seqs, output_seqs = create_input_output(all_notes)\n",
        "\n",
        "print(\"Input Sequences\")\n",
        "print(type(input_seqs))\n",
        "print(input_seqs.shape)\n",
        "\n",
        "print(\"Output Sequences\")\n",
        "print(type(output_seqs))\n",
        "print(output_seqs.shape)\n",
        "\n",
        "del all_notes,df "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjuLT6FND0vX"
      },
      "source": [
        "### Creating a train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PDIIDr34UBT0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into train and test sets (80% train, 20% test)\n",
        "x_train, x_test, y_train, y_test = train_test_split(input_seqs, output_seqs, test_size=0.2)\n",
        "\n",
        "del input_seqs, output_seqs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7jud_yGfG2F"
      },
      "source": [
        "# Designing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qruHFP0cUBT0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import LSTM, Dense, Dropout, Input\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Lambda\n",
        "import keras.backend as K\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRZeSILCUBT0",
        "outputId": "c0a71aab-3dee-40be-a93f-6ba9d0e3c930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 25, 3)]      0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 128)          67584       ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " duration (Dense)               (None, 1)            129         ['lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " pitch_probs (Dense)            (None, 128)          16512       ['lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " step (Dense)                   (None, 1)            129         ['lstm[0][0]']                   \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 84,354\n",
            "Trainable params: 84,354\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# create a LSTM model using input_seqs and output_seqs\n",
        "\n",
        "input_shape = (25, 3)\n",
        "learning_rate = 0.005\n",
        "\n",
        "inputs = tf.keras.Input(input_shape)\n",
        "x = tf.keras.layers.LSTM(128)(inputs)\n",
        "\n",
        "# 3 elements of the note output\n",
        "pitch_probs = tf.keras.layers.Dense(128, activation=\"softmax\", name='pitch_probs')(x) # returns softmax \"prob\" for each pitch number\n",
        "step = tf.keras.layers.Dense(1, activation=\"linear\",name='step')(x)\n",
        "duration = tf.keras.layers.Dense(1, activation=\"linear\",name='duration')(x)\n",
        "\n",
        "# combining three elements \n",
        "# outputs = {'pitch': pitch, 'step': step, 'duration': duration}\n",
        "outputs = {'pitch': pitch_probs, 'step': step, 'duration': duration}\n",
        "\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "loss = {'pitch': \"sparse_categorical_crossentropy\", 'step': \"mse\", 'duration': \"mse\"}\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(\n",
        "    loss=loss,\n",
        "    loss_weights={\n",
        "        'pitch': 0.05,\n",
        "        'step': 1.0,\n",
        "        'duration':1.0,\n",
        "    },\n",
        "    optimizer=optimizer,\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCiWfBFXDy57"
      },
      "source": [
        "# Training the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQHqqCkILGXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e30877a5-df8f-4e69-8c2a-ce0d079dced2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "33722/88449 [==========>...................] - ETA: 5:36 - loss: 0.3483 - duration_loss: 0.1014 - pitch_probs_loss: 3.8588 - step_loss: 0.0539"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='loss',\n",
        "        patience=5,\n",
        "        verbose=1,\n",
        "        restore_best_weights=True),\n",
        "]\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "# create labels for each output\n",
        "pitch_labels = y_train[:, 0].astype(int)\n",
        "step_labels = y_train[:, 1].reshape(-1, 1)\n",
        "duration_labels = y_train[:, 2].reshape(-1, 1)\n",
        "\n",
        "# batch size\n",
        "batch_size = 16\n",
        "\n",
        "# train the model\n",
        "history = model.fit(\n",
        "    x=x_train,\n",
        "    y={\"pitch\": pitch_labels, \"step\": step_labels, \"duration\": duration_labels}, \n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "# save model after running\n",
        "model.save('model.h5')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmKnzOAJDr-_"
      },
      "source": [
        "Analyzing its accuracy over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3ltaItQ3m2F"
      },
      "source": [
        "# Generate New Music!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5ZNjvW6__4n"
      },
      "outputs": [],
      "source": [
        "def generate_next_note(input_sequence, model, temperature):\n",
        "    # Make sure the input sequence has the correct shape\n",
        "    if len(input_sequence) < 25:\n",
        "        input_sequence = np.pad(input_sequence, ((25-len(input_sequence), 0), (0, 0)), 'constant')\n",
        "\n",
        "    # Convert the input sequence to a format suitable for the model\n",
        "    input_sequence = np.array([input_sequence])\n",
        "\n",
        "    # Generate the output probabilities using the model\n",
        "    output_probs = model.predict(input_sequence)\n",
        "\n",
        "    # Modify the pitch probabilities using the temperature parameter\n",
        "    pitch_probs = output_probs[\"pitch\"][0] # Extract the pitch probabilities from the output\n",
        "    pitch_probs = np.log(pitch_probs) / temperature\n",
        "    pitch_probs = np.exp(pitch_probs) / np.sum(np.exp(pitch_probs))\n",
        "\n",
        "    # Sample the next pitch from the modified probabilities\n",
        "    pitch_index = np.random.choice(len(pitch_probs), p=pitch_probs)\n",
        "    next_pitch = pitch_index\n",
        "\n",
        "    # Modify the step and duration predictions using the temperature parameter\n",
        "    step_mean = output_probs[\"step\"][0][0]\n",
        "    step_stddev = np.exp(output_probs[\"step\"][0][0] / temperature)\n",
        "    next_step = np.random.normal(loc=step_mean, scale=step_stddev)\n",
        "\n",
        "    duration_mean = output_probs[\"duration\"][0][0]\n",
        "    duration_stddev = np.exp(output_probs[\"duration\"][0][0] / temperature)\n",
        "    next_duration = np.random.normal(loc=duration_mean, scale=duration_stddev)\n",
        "\n",
        "    return [next_pitch, next_step, next_duration]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wL31KKh2UBT1"
      },
      "outputs": [],
      "source": [
        "sample_input = x_test[0]\n",
        "\n",
        "print(\"Sample input:\")\n",
        "print(sample_input[:5])\n",
        "print(\"Sample input shape\",sample_input.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_note = generate_next_note(sample_input,model,5)\n",
        "print(new_note)"
      ],
      "metadata": {
        "id": "nMQtUcfxe5bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_music(model,input,length,temperature):\n",
        "  new_song = []\n",
        "  prev_start = 0\n",
        "  for i in range(length):\n",
        "    # generating a new note with the model\n",
        "    next_note = generate_next_note(sample_input,model,temperature)\n",
        "\n",
        "    # details of new note\n",
        "    pitch = next_note[0]\n",
        "    step = next_note[1]\n",
        "    duration = next_note[2]\n",
        "    \n",
        "    start = prev_start + step\n",
        "    end = start + duration\n",
        "    prev_start = start\n",
        "  \n",
        "    new_song.append([pitch,step,duration,start,end])\n",
        "\n",
        "  # plot_piano_roll(new_song)\n",
        "\n",
        "  song_df = pd.DataFrame(\n",
        "      data=new_song,\n",
        "      columns=[\"pitch\",\"step\",\"duration\",\"start\",\"end\"]\n",
        "  )\n",
        "\n",
        "  return song_df"
      ],
      "metadata": {
        "id": "55yHmz2qVfER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "song = generate_music(model,sample_input,1000,1)\n",
        "print(\"Input\")\n",
        "print(song[:25])\n",
        "print(\"New Music\")\n",
        "print(song[25:])"
      ],
      "metadata": {
        "id": "R26r4akFmS1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "song"
      ],
      "metadata": {
        "id": "vqzLwYhAmVmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_piano_roll(song)"
      ],
      "metadata": {
        "id": "DUvQWueisIfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def notes_to_midi(\n",
        "  notes: pd.DataFrame,\n",
        "  out_file: str, \n",
        "  instrument = pretty_midi.Instrument(program=0),\n",
        "  velocity: int = 100,  # note loudness\n",
        ") -> pretty_midi.PrettyMIDI:\n",
        "\n",
        "  # source - https://www.tensorflow.org/tutorials/audio/music_generation\n",
        "\n",
        "  pm = pretty_midi.PrettyMIDI()\n",
        "  # instrument = pretty_midi.Instrument(\n",
        "  #     program=pretty_midi.instrument_name_to_program(\n",
        "  #         instrument_name))\n",
        "\n",
        "  prev_start = 0\n",
        "  for i, note in notes.iterrows():\n",
        "    start = float(prev_start + note['step'])\n",
        "    end = float(start + note['duration'])\n",
        "    note = pretty_midi.Note(\n",
        "        velocity=velocity,\n",
        "        pitch=int(note['pitch']),\n",
        "        start=start,\n",
        "        end=end,\n",
        "    )\n",
        "    instrument.notes.append(note)\n",
        "    prev_start = start\n",
        "\n",
        "  pm.instruments.append(instrument)\n",
        "  pm.write(out_file)\n",
        "  return pm"
      ],
      "metadata": {
        "id": "i4UZTc8js0as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pm = notes_to_midi(song,\"song.mid\")\n",
        "pm"
      ],
      "metadata": {
        "id": "FdoCkCCxte9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ZEECmD5ths3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}